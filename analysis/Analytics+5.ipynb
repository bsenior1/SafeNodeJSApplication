{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://central.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar\n",
      "Finished download of gson-2.2.4.jar\n",
      "Starting download from https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.1.1/org.eclipse.paho.client.mqttv3-1.1.1.jar\n",
      "Finished download of org.eclipse.paho.client.mqttv3-1.1.1.jar\n",
      "Starting download from https://github.com/sathipal/spark-streaming-mqtt-with-security_2.10-1.3.0/releases/download/0.0.1/spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar\n",
      "Finished download of spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar\n",
      "Starting download from http://central.maven.org/maven2/org/scalanlp/breeze_2.10/0.12/breeze_2.10-0.12.jar\n",
      "Finished download of breeze_2.10-0.12.jar\n",
      "Starting download from http://central.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.0.0-rc1/cassandra-driver-core-3.0.0-rc1.jar\n",
      "Finished download of cassandra-driver-core-3.0.0-rc1.jar\n",
      "Starting download from http://dl.bintray.com/spark-packages/maven/datastax/spark-cassandra-connector/1.6.0-s_2.10/spark-cassandra-connector-1.6.0-s_2.10.jar\n",
      "Finished download of spark-cassandra-connector-1.6.0-s_2.10.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://central.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar -f\n",
    "%AddJar https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.1.1/org.eclipse.paho.client.mqttv3-1.1.1.jar -f\n",
    "%AddJar https://github.com/sathipal/spark-streaming-mqtt-with-security_2.10-1.3.0/releases/download/0.0.1/spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar -f \n",
    "%AddJar http://central.maven.org/maven2/org/scalanlp/breeze_2.10/0.12/breeze_2.10-0.12.jar -f\n",
    "%AddJar http://central.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.0.0-rc1/cassandra-driver-core-3.0.0-rc1.jar -f\n",
    "%AddJar http://dl.bintray.com/spark-packages/maven/datastax/spark-cassandra-connector/1.6.0-s_2.10/spark-cassandra-connector-1.6.0-s_2.10.jar -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import scala.math.BigDecimal\n",
    "import scala.io.Source\n",
    "import org.eclipse.paho.client.mqttv3._\n",
    "import org.eclipse.paho.client.mqttv3.persist.MemoryPersistence\n",
    "import com.google.gson.JsonObject\n",
    "import com.google.gson.JsonParser\n",
    "\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.streaming.mqtt._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "import java.util.Map.Entry\n",
    "import com.google.gson.JsonElement\n",
    "import java.util.Set\n",
    "\n",
    "import scala.collection.mutable.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "import com.datastax.spark.connector._\n",
    "\n",
    "import breeze.linalg._\n",
    "\n",
    "\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql._\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import scala.math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class JsonPackage() extends Serializable {\n",
    "\n",
    "    \n",
    "    var  _event : String  = \"\"\n",
    "    var  _timestamp : String  = \"\"\n",
    "    var  _severity : Int  = 0\n",
    "    var  _temp : Int  = 0\n",
    "    var  _precip : String = \"\"\n",
    "    var  _construction : Int = 0\n",
    "    var  _tweet : String = \"\"\n",
    "    \n",
    "     \n",
    "    def event_= (value:String):Unit = _event = value  \n",
    "    def event = _event\n",
    "    \n",
    "    def timestamp_= (value:String):Unit = _timestamp = value  \n",
    "    def timestamp = _timestamp\n",
    "  \n",
    "    def temp_= (value:Int):Unit = _temp = value  \n",
    "    def temp = _temp\n",
    "    \n",
    "    def precip_= (value:String):Unit = _precip = value  \n",
    "    def precip = _precip\n",
    "    \n",
    "    def tweet_= (value:String):Unit = _tweet = value  \n",
    "    def tweet = _tweet\n",
    "    \n",
    "    override def toString: String =\n",
    "    {\n",
    "        \n",
    "        var contents = \"<Package>\\n\\t<Event>\"+_event+\"<\\\\Event>\\n\\t<Temp>\"+_temp+\"<\\\\Temp>\\n\\t<Precip>\"+_precip+\"<\\\\Precip>\\n\\t<Tweet>\"+_tweet+\"<\\\\Tweet>\" \n",
    "        var closingtags = \"\\n<\\\\Package>\"\n",
    "        var value = contents+closingtags\n",
    "        \n",
    "        value\n",
    "    }\n",
    "    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1493324410000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324415000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324420000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324425000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324430000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324435000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324440000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324445000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324450000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324455000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324460000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493324465000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@transient val ssc = new StreamingContext(sc, Seconds(5))\n",
    "ssc.checkpoint(\".\")\n",
    "\n",
    "@transient val lines = MQTTUtils.createStream(ssc,\n",
    "\"ssl://hbp995.messaging.internetofthings.ibmcloud.com:8883\", // Watson IoT Platform URL\n",
    "                \"iot-2/type/+/id/+/evt/+/fmt/+\",                                // MQTT topic to receive the events\n",
    "                \"a:wkabcg:random\",                                          // Unique ID of the application\n",
    "                \"a-wkabcg-ipdatxnzur\",                                                    // API-Key\n",
    "                \"AD&0GgP+)pXMoO)DJc\")    \n",
    "\n",
    "@transient  val deviceMappedLines = lines.map(x => ((x.split(\" \", 2)(0)).split(\"/\")(4), x.split(\" \", 2)(1)))\n",
    "deviceMappedLines.print()\n",
    "\n",
    "@transient val batch = deviceMappedLines.map(x => {\n",
    "    var json = new JsonPackage()\n",
    "\tval payload = new JsonParser().parse(x._2).getAsJsonObject()\n",
    "    val deviceObject = payload.get(\"d\").getAsJsonObject()\n",
    "\tval setObj = deviceObject.entrySet()\n",
    "    val itr = setObj.iterator()\n",
    "    while(itr.hasNext()) {\n",
    " \t\tval entry = itr.next()\n",
    "        var index = entry.toString.split(\"=\")(0)\n",
    "        if(index == \"type\")\n",
    "        {\n",
    "           json.event = entry.getValue.getAsString() \n",
    "        }\n",
    "        if(index == \"tweet\")\n",
    "        {\n",
    "           json.tweet = entry.getValue.getAsString() \n",
    "        }\n",
    "        if(index == \"temp\")\n",
    "        {\n",
    "           json.temp = entry.getValue.getAsInt() \n",
    "        }\n",
    "        if(index == \"precip\")\n",
    "        {\n",
    "           json.precip = entry.getValue.getAsString() \n",
    "        }\n",
    "\n",
    "      \n",
    "    }\n",
    "\n",
    "    json\n",
    "})\n",
    "\n",
    "batch.foreachRDD(rdd => rdd.collect().foreach{json =>\n",
    "                                              \n",
    "   println(json.toString())\n",
    "  })\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}